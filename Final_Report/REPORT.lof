\contentsline {figure}{\numberline {1}{\ignorespaces The robot from EE303. This includes the two DC motors, microcontroller and sensor array.}}{13}{figure.1}%
\contentsline {figure}{\numberline {2}{\ignorespaces The track from EE303 web server showing the complex radii, intersections and location numbers \blx@tocontentsinit {0}\cite {KevinMcGuinness}}}{13}{figure.2}%
\contentsline {figure}{\numberline {3}{\ignorespaces Reinforcement Learning Model Structure where $T$ is the environment, $B$ is the agent, $a$ is the action space and $s$ is the state space. The state space is then split into $I$ the observation space and $R$ the reward \blx@tocontentsinit {0}\cite {kaelbling1996reinforcement}}}{15}{figure.3}%
\contentsline {figure}{\numberline {4}{\ignorespaces Gym Hero's user interface. Created to mimic the UI of Guitar Hero \blx@tocontentsinit {0}\cite {GymHero}.}}{19}{figure.4}%
\contentsline {figure}{\numberline {5}{\ignorespaces Doodle Jumps graphics including player,all platforms and obstacles \blx@tocontentsinit {0}\cite {DoodleJump}.}}{21}{figure.5}%
\contentsline {figure}{\numberline {6}{\ignorespaces All of the Panda environments for reinforcement learning. Robot has seven degrees of freedom.}}{23}{figure.6}%
\contentsline {figure}{\numberline {7}{\ignorespaces The graphics of the car racing environment including the car and the randomised track \blx@tocontentsinit {0}\cite {Klimov}.}}{25}{figure.7}%
\contentsline {figure}{\numberline {8}{\ignorespaces EE303's line follower robot simulated in a PyBullet environment}}{28}{figure.8}%
\contentsline {figure}{\numberline {9}{\ignorespaces Image of the bottom of EE303's line following robot showing the two motors, caster wheel and sensor array}}{30}{figure.9}%
\contentsline {figure}{\numberline {10}{\ignorespaces Diagram of the robot showing each of the variables that dictate the current and future locations}}{31}{figure.10}%
\contentsline {figure}{\numberline {11}{\ignorespaces Diagram to show how Pygame detects collisions using an image's vertical and horizontal edges}}{32}{figure.11}%
\contentsline {figure}{\numberline {12}{\ignorespaces Diagram showing how a rectangular image can be converted into a mask for pixel-perfect collision detection}}{32}{figure.12}%
\contentsline {figure}{\numberline {13}{\ignorespaces EE303's track showing all of the complex radii and intersections \blx@tocontentsinit {0}\cite {KevinMcGuinness}}}{33}{figure.13}%
\contentsline {figure}{\numberline {14}{\ignorespaces The circular nature of reinforcment learning. This shows how the agent uses its observation to make a decision and be rewarded}}{34}{figure.14}%
\contentsline {figure}{\numberline {15}{\ignorespaces The initial observation space including the robot's position, orientation, velocity and present sensor values}}{35}{figure.15}%
\contentsline {figure}{\numberline {16}{\ignorespaces A simplified observation space simplified down to just the current sensor values}}{35}{figure.16}%
\contentsline {figure}{\numberline {17}{\ignorespaces Simplified version of EE303's track used to develop a working observation space \blx@tocontentsinit {0}\cite {KevinMcGuinness}}}{36}{figure.17}%
\contentsline {figure}{\numberline {18}{\ignorespaces The final observation space including just thirty sensor values. One present set and five historic sets of values}}{36}{figure.18}%
\contentsline {figure}{\numberline {19}{\ignorespaces Pseudocode for Initial Reward System}}{37}{figure.19}%
\contentsline {figure}{\numberline {20}{\ignorespaces Pseudocode for Further Reward System}}{38}{figure.20}%
\contentsline {figure}{\numberline {21}{\ignorespaces Pseudocode for Final Reward System}}{38}{figure.21}%
\contentsline {figure}{\numberline {22}{\ignorespaces Final environment developed using PyGame}}{39}{figure.22}%
\contentsline {figure}{\numberline {23}{\ignorespaces TensorBoard graphs for mean duration over 2.5 million timesteps}}{40}{figure.23}%
\contentsline {figure}{\numberline {24}{\ignorespaces TensorBoard graphs for mean reward over 2.5 million timesteps}}{41}{figure.24}%
\contentsline {figure}{\numberline {25}{\ignorespaces Arcitecture of the trained PPO neural network viewed using Netron}}{42}{figure.25}%
\contentsline {figure}{\numberline {26}{\ignorespaces TensorBoard graph for more than one hundred million timesteps. Graph showing the mean simulation time against the number of timesteps}}{46}{figure.26}%
\contentsline {figure}{\numberline {27}{\ignorespaces TensorBoard graph for more than one hundred million timesteps. Graph showing the mean reward against the number of timesteps}}{46}{figure.27}%
\contentsline {figure}{\numberline {28}{\ignorespaces TensorBoard graph comparing different training graphs against each other. Comparing the different mean lengths of simulation over their training}}{49}{figure.28}%
\contentsline {figure}{\numberline {29}{\ignorespaces TensorBoard graph comparing different training graphs against each other. Comparing the different mean rewards over their training}}{49}{figure.29}%
\contentsline {figure}{\numberline {30}{\ignorespaces TensorBoard showing the training progress of a discrete action spaced neural network using Proximal Policy Optimization. Showing the mean length of simulation against the total timesteps }}{52}{figure.30}%
\contentsline {figure}{\numberline {31}{\ignorespaces TensorBoard showing the training progress of a discrete action spaced neural network using Proximal Policy Optimization. Showing the mean reward against the total timesteps }}{53}{figure.31}%
